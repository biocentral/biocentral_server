services:
  biocentral-server:
    image: ghcr.io/biocentral/biocentral_server/biocentral-server:latest
    ports:
      - "9540:9540"
    volumes:
      - huggingface_models_cache:/app/huggingface_models
      - autoeval_storage:/app/autoeval
    environment:
      - MAX_FILE_SIZE=${MAX_FILE_SIZE}
      - POSTGRES_HOST=embeddings-db
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - HF_HOME=/app/huggingface_models
      - AUTOEVAL_DATA_DIR=/app/autoeval
      - TRITON_GRPC_URL=http://triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      - USE_TRITON=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      biocentral-worker:
        condition: service_started
      redis-jobs:
        condition: service_healthy
      embeddings-db:
        condition: service_healthy
      seaweedfs-filer:
        condition: service_healthy
      triton:
        condition: service_healthy
    networks:
      - biocentral-network

  biocentral-worker:
    image: ghcr.io/biocentral/biocentral_server/biocentral-server:latest
    command: rq worker-pool --url redis://redis-jobs:6379/0 high default low -n 3
    volumes:
      - huggingface_models_cache:/app/huggingface_models
      - autoeval_storage:/app/autoeval
    environment:
      - POSTGRES_HOST=embeddings-db
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - HF_HOME=/app/huggingface_models
      - AUTOEVAL_DATA_DIR=/app/autoeval
      - TRITON_GRPC_URL=http://triton:8001
      - TRITON_HTTP_URL=http://triton:8000
      - USE_TRITON=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    depends_on:
      redis-jobs:
        condition: service_healthy
      embeddings-db:
        condition: service_healthy
      seaweedfs-filer:
        condition: service_healthy
      triton:
        condition: service_healthy
    networks:
      - biocentral-network

  redis-jobs:
    image: redis:alpine
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - biocentral-network

  embeddings-db:
    image: postgres:15
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - biocentral-network

  seaweedfs-master:
    image: chrislusf/seaweedfs
    command: "master -ip=seaweedfs-master -port=9333 -port.grpc=19333"
    networks:
      - biocentral-network

  seaweedfs-volume:
    image: chrislusf/seaweedfs
    command: 'volume -dir="/data" -max=5 -mserver="seaweedfs-master:9333" -port=8080 -port.grpc=18080 -ip=seaweedfs-volume'
    volumes:
      - seaweedfs_data:/data
    depends_on:
      - seaweedfs-master
    networks:
      - biocentral-network

  seaweedfs-filer:
    image: chrislusf/seaweedfs
    command: 'filer -master="seaweedfs-master:9333" -port=8888 -port.grpc=18888 -ip=seaweedfs-filer'
    depends_on:
      - seaweedfs-master
      - seaweedfs-volume
    networks:
      - biocentral-network
    healthcheck:
      test: [ "CMD", "wget", "--spider", "--quiet", "-Y", "off", "http://seaweedfs-filer:8888/" ]
      interval: 5s
      timeout: 5s
      retries: 20
      start_period: 30s

  triton-model-init:
    build:
      context: ./storage/model-repository
      dockerfile: Dockerfile.init
    volumes:
      - triton_models:/models
    environment:
      - MODEL_REPOSITORY_PATH=/models
      - MODEL_DOWNLOADS=${MODEL_DOWNLOADS:-}
    networks:
      - biocentral-network

  triton:
    build:
      context: ./storage/model-repository
      dockerfile: Dockerfile.triton
    deploy:
      resources:
        limits:
          memory: 24G  # ESM2-T36 (3B params) requires ~14-22GB for inference
        reservations:
          memory: 16G
    command: >
      tritonserver
      --model-repository=/models
      --model-control-mode=explicit
      --load-model=prot_t5_pipeline
      --load-model=esm2_t33_pipeline
      --load-model=esm2_t36_pipeline
      --load-model=prott5_sec
      --load-model=prott5_cons
      --load-model=bind_embed
      --load-model=seth
      --load-model=tmbed
      --load-model=light_attention_subcell
      --load-model=light_attention_membrane
      --load-model=vespag
      --disable-auto-complete-config
      --log-verbose=1
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ./storage/model-repository:/models:ro
    depends_on:
      triton-model-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${EMBEDDINGS_DATA_DIR}
  seaweedfs_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${FILES_DATA_DIR}
  temp_server_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${SERVER_TEMP_DATA_DIR}
  huggingface_models_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HUGGINGFACE_MODELS_DIR}
  autoeval_storage:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${AUTOEVAL_DATA_DIR}
  triton_models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${MODEL_REPOSITORY_PATH:-./storage/model-repository}
networks:
  biocentral-network:
    driver: bridge
